@ARTICLE{9313993,
  author={Chen, Yaru and Gu, Xiaohong and Zhou, Conghua and Zhu, Xiaolong and Jiang, Yi and Arthur, John Kingsley and Mantey, Eric Appiah and Ganaa, Ernest Domanaanmwi},
  journal={IEEE Access}, 
  title={A Novel Hierarchical Deep Matrix Completion Method}, 
  year={2021},
  volume={9},
  number={},
  pages={7908-7920},
  abstract={The matrix completion technique based on matrix factorization for recovering missing items is widely used in collaborative filtering, image restoration, and other applications. We proposed a new matrix completion model called hierarchical deep matrix completion (HDMC), where we assume that the variables lie in hierarchically organized groups. HDMC explicitly expresses either shallow or high-level hierarchical structures, such as taxonomy trees, by embedding a series of so-called structured sparsity penalties in a framework to encourage hierarchical relations between compact representations and reconstructed data. Moreover, HDMC considers the group-level sparsity of neurons in a neural network to obtain a pruning effect and compact architecture by enhancing the relevance of within-group neurons while neglecting the between-group neurons. Since the optimization of HDMC is a nonconvex problem, to avoid converting the framework of the HDMC models into separate optimized formulations, we unify a generic optimization by applying a smoothing proximal gradient strategy in dual space. HDMC is compared with state-of-the-art matrix completion methods on applications with simulated data, MRI image datasets, and gene expression datasets. The experimental results verify that HDMC achieves higher matrix completion accuracy.},
  keywords={Neurons;Taxonomy;Matrix converters;Optimization;Indexes;Image reconstruction;Diseases;Matrix completion;hierarchical relation;structured sparsity;regulation;neural network},
  doi={10.1109/ACCESS.2021.3049297},
  ISSN={2169-3536},
  month={},}@ARTICLE{9208795,
  author={Laguarta, Jordi and Hueto, Ferran and Subirana, Brian},
  journal={IEEE Open Journal of Engineering in Medicine and Biology}, 
  title={COVID-19 Artificial Intelligence Diagnosis Using Only Cough Recordings}, 
  year={2020},
  volume={1},
  number={},
  pages={275-281},
  abstract={Goal: We hypothesized that COVID-19 subjects, especially including asymptomatics, could be accurately discriminated only from a forced-cough cell phone recording using Artificial Intelligence. To train our MIT Open Voice model we built a data collection pipeline of COVID-19 cough recordings through our website (opensigma.mit.edu) between April and May 2020 and created the largest audio COVID-19 cough balanced dataset reported to date with 5,320 subjects. Methods: We developed an AI speech processing framework that leverages acoustic biomarker feature extractors to pre-screen for COVID-19 from cough recordings, and provide a personalized patient saliency map to longitudinally monitor patients in real-time, non-invasively, and at essentially zero variable cost. Cough recordings are transformed with Mel Frequency Cepstral Coefficient and inputted into a Convolutional Neural Network (CNN) based architecture made up of one Poisson biomarker layer and 3 pre-trained ResNet50's in parallel, outputting a binary pre-screening diagnostic. Our CNN-based models have been trained on 4256 subjects and tested on the remaining 1064 subjects of our dataset. Transfer learning was used to learn biomarker features on larger datasets, previously successfully tested in our Lab on Alzheimer's, which significantly improves the COVID-19 discrimination accuracy of our architecture. Results: When validated with subjects diagnosed using an official test, the model achieves COVID-19 sensitivity of 98.5% with a specificity of 94.2% (AUC: 0.97). For asymptomatic subjects it achieves sensitivity of 100% with a specificity of 83.2%. Conclusions: AI techniques can produce a free, non-invasive, real-time, any-time, instantly distributable, large-scale COVID-19 asymptomatic screening tool to augment current approaches in containing the spread of COVID-19. Practical use cases could be for daily screening of students, workers, and public as schools, jobs, and transport reopen, or for pool testing to quickly alert of outbreaks in groups. General speech biomarkers may exist that cover several disease categories, as we demonstrated using the same ones for COVID-19 and Alzheimer's.},
  keywords={Biological system modeling;Artificial intelligence;Tools;Testing;Monitoring;Sensitivity;Degradation;AI diagnostics;convolutional neural networks;COVID-19 screening;deep learning;speech recognition},
  doi={10.1109/OJEMB.2020.3026928},
  ISSN={2644-1276},
  month={},}@ARTICLE{9861597,
  author={Liu, Jia and Singh, Avinash Kumar and Lin, Chin-Teng},
  journal={IEEE Transactions on Neural Systems and Rehabilitation Engineering}, 
  title={Predicting the Quality of Spatial Learning via Virtual Global Landmarks}, 
  year={2022},
  volume={30},
  number={},
  pages={2418-2425},
  abstract={Analyzing the effects landmarks have on spatial learning is an active area of research in the study of human navigation processes and one that is key to understanding the links between human brain dynamics, landmark encoding, and spatial learning outcomes. This article presents a study on whether electroencephalography (EEG) signals related to virtual global landmarks combined with deep learning can be used to predict the accuracy and efficacy of spatial learning. Virtual global landmarks are silhouettes of actual landmarks projected into the navigator’s vision via a heads-up display. They serve as a notable frame of reference in addition to the local landmarks we all typically use for route navigation. From a mobile virtual reality scenario involving 55 participants, the results of the study suggest that the EEG data associated with those who were exposed to global landmarks shows a visibly better capacity for predicting the quality of spatial learning levels than those who were not. As such, the EEG features associated with processing VGLs have a greater functional relation to the quality of spatial learning. This finding opens up a future direction of enquiry into landmark encoding and navigational ability. It may also provide a potential avenue for the early diagnosis of Alzheimer’s disease.},
  keywords={Electroencephalography;Navigation;Brain modeling;Task analysis;Encoding;Computational modeling;Australia;Active navigation;electroencephalography (EEG);spatial learning;deep learning},
  doi={10.1109/TNSRE.2022.3199713},
  ISSN={1558-0210},
  month={},}@ARTICLE{10353995,
  author={Amer, Nisreen Said and Belhaouari, Samir Brahim},
  journal={IEEE Access}, 
  title={EEG Signal Processing for Medical Diagnosis, Healthcare, and Monitoring: A Comprehensive Review}, 
  year={2023},
  volume={11},
  number={},
  pages={143116-143142},
  abstract={EEG is a common and safe test that uses small electrodes to record electrical signals from the brain. It has a broad range of applications in medical diagnosis, including diagnosis of epileptic seizure, Alzheimer’s, brain tumors, head injury, sleep disorders, stroke, and other seizure and neurological disorders. EEG can also be used to help diagnose death in people who are in a persistent coma. The use of digital signal processing and machine learning to improve EEG analysis for medical diagnosis has gained traction in recent years. This is because EEG visual analysis can be complex and time-consuming, as it mostly involves high dimensions and consists of large datasets. The development of novel sensors for EEG recording, digital signal processing algorithms, feature engineering, and detection algorithms increases the need for efficient diagnostic systems. An extensive review of the recent approaches for EEG preprocessing, extraction of features, and diagnosis of brain disorders is provided. In this paper, the main focus is to identify reliable algorithms for preprocessing, feature engineering, and classification of EEG, applied to medical healthcare and diagnosis, providing practitioners with insights into the most effective strategies, as well as potential future directions for improving accuracy of the automatic diagnostic systems. The study of reliable feature extraction and classification algorithms is crucial for a more accurate analysis of EEG signals. This paper can provide valuable information to researchers and practitioners working in the fields of EEG analysis and machine learning, as it provides a summary of recent developments and highlights key areas for future research. This paper can help researchers and clinicians to stay up-to-date on the latest developments in this field.},
  keywords={Electroencephalography;Feature extraction;Electrodes;Neurons;Medical diagnosis;Epilepsy;Classification algorithms;Machine learning;Classification;electroencephalogram (EEG);feature extraction;machine learning;preprocessing},
  doi={10.1109/ACCESS.2023.3341419},
  ISSN={2169-3536},
  month={},}@ARTICLE{9552865,
  author={Hoffmann, Malte and Billot, Benjamin and Greve, Douglas N. and Iglesias, Juan Eugenio and Fischl, Bruce and Dalca, Adrian V.},
  journal={IEEE Transactions on Medical Imaging}, 
  title={SynthMorph: Learning Contrast-Invariant Registration Without Acquired Images}, 
  year={2022},
  volume={41},
  number={3},
  pages={543-558},
  abstract={We introduce a strategy for learning image registration without acquired imaging data, producing powerful networks agnostic to contrast introduced by magnetic resonance imaging (MRI). While classical registration methods accurately estimate the spatial correspondence between images, they solve an optimization problem for every new image pair. Learning-based techniques are fast at test time but limited to registering images with contrasts and geometric content similar to those seen during training. We propose to remove this dependency on training data by leveraging a generative strategy for diverse synthetic label maps and images that exposes networks to a wide range of variability, forcing them to learn more invariant features. This approach results in powerful networks that accurately generalize to a broad array of MRI contrasts. We present extensive experiments with a focus on 3D neuroimaging, showing that this strategy enables robust and accurate registration of arbitrary MRI contrasts even if the target contrast is not seen by the networks during training. We demonstrate registration accuracy surpassing the state of the art both within and across contrasts, using a single model. Critically, training on arbitrary shapes synthesized from noise distributions results in competitive performance, removing the dependency on acquired data of any kind. Additionally, since anatomical label maps are often available for the anatomy of interest, we show that synthesizing images from these dramatically boosts performance, while still avoiding the need for real intensity images. Our code is available at doic https://w3id.org/synthmorph.},
  keywords={Training;Magnetic resonance imaging;Strain;Three-dimensional displays;Shape;Neuroimaging;Measurement;Deformable image registration;data independence;deep learning;MRI-contrast invariance},
  doi={10.1109/TMI.2021.3116879},
  ISSN={1558-254X},
  month={March},}@ARTICLE{9369406,
  author={Zia, Shafaq and Khan, Ali Nawaz and Zaidi, Khurram Shabih and Ali, Shan E},
  journal={IEEE Access}, 
  title={Detection of Generalized Tonic Clonic Seizures and Falls in Unconstraint Environment Using Smartphone Accelerometer}, 
  year={2021},
  volume={9},
  number={},
  pages={39432-39443},
  abstract={The detection of Generalized Tonic Clonic Seizures (GTCS) and Falls is of utmost importance due to the increase in prevalence of epilepsy and Sudden Death in Epileptic Patients during CoVID-19 pandemic, and prevention of serious injuries in Fall risk groups such as elderly requiring continuous monitoring for disease management and assisted living etc. Monitoring of Activities of Daily Living (ADLs) can assist in the detection of symptoms and onset of neurological disorders such as Alzheimer’s, stroke, and epileptic seizures. With a host of embedded sensors, improved memory, enhanced processing capabilities and availability to masses, smartphones can be used for Human Activity Recognition (HAR) through continuous monitoring of ADLs. This paper presents a tri-axial accelerometer-based approach to detect and classify activities performed by individuals by applying machine learning algorithms including RF, J48, NB, LMT and SVM to movement data. Movement data is collected in real-time from the embedded accelerometer of a smartphone worn by individual on upper-left arm in unconstraint environment. It is pre-processed using time and frequency domain analysis and spatial domain features are computed. Supervised machine learning techniques are applied to classify ADLs into five classes based on the intensity of movements: Stationary, Light Ambulatory, Intense Ambulatory, GTCS and Falls. We also used training data from MyNeuroHealth dataset collected from 23 individuals including epilepsy patients. Based on gathered results, Random Forest outperforms other classifiers with classification accuracy of 99.6% for stationary, 81.5% for light ambulatory, 99.8% for intense ambulatory and GTCS, and 97.2% for Falls corresponding to training data of 14000 samples. To date, activity classification in our system has been implemented on cloud instead of mobile phone application as subjects are using smartphones with dissimilar software and hardware specifications for assisted living applications.},
  keywords={Sensors;Accelerometers;Wearable sensors;Sensor phenomena and characterization;Feature extraction;Support vector machines;Neurological diseases;Accelerometer;activity recognition;biomedical signal processing;assisted living;machine learning},
  doi={10.1109/ACCESS.2021.3063765},
  ISSN={2169-3536},
  month={},}@ARTICLE{10453042,
  author={Mishra, Pratik K. and Mihailidis, Alex and Khan, Shehroz S.},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence}, 
  title={Skeletal Video Anomaly Detection Using Deep Learning: Survey, Challenges, and Future Directions}, 
  year={2024},
  volume={8},
  number={2},
  pages={1073-1085},
  abstract={The existing methods for video anomaly detection mostly utilize videos containing identifiable facial and appearance-based features. The use of videos with identifiable faces raises privacy concerns, especially when used in a hospital or community-based setting. Appearance-based features can also be sensitive to pixel-based noise, straining the anomaly detection methods to model the changes in the background and making it difficult to focus on the actions of humans in the foreground. Structural information in the form of skeletons describing the human motion in the videos is privacy-protecting and can overcome some of the problems posed by appearance-based features. In this paper, we present a survey of privacy-protecting deep learning anomaly detection methods using skeletons extracted from videos. We present a novel taxonomy of algorithms based on the various learning approaches. We conclude that skeleton-based approaches for anomaly detection can be a plausible privacy-protecting alternative for video anomaly detection. Lastly, we identify major open research questions and provide guidelines to address them.},
  keywords={Skeleton;Anomaly detection;Cameras;Pedestrians;Taxonomy;Generators;Skeleton;body joint;human pose;anomaly detection;video},
  doi={10.1109/TETCI.2024.3358103},
  ISSN={2471-285X},
  month={April},}@ARTICLE{9151874,
  author={Gonzalez, Hector A. and Muzaffar, Shahzad and Yoo, Jerald and Elfadel, Ibrahim M.},
  journal={IEEE Access}, 
  title={BioCNN: A Hardware Inference Engine for EEG-Based Emotion Detection}, 
  year={2020},
  volume={8},
  number={},
  pages={140896-140914},
  abstract={EEG-based emotion classifiers have the potential of significantly improving the social integration of patients suffering from neurological disorders such as Amyotrophic Lateral Sclerosis or the acute stages of Alzheimer's disease. Emotion classifiers have historically used software on general-purpose computers and operating under off-line conditions. Yet the wearability of such classifiers is a must if they are to enable the socialization of critical-care patients. Such wearability requires the use of low-power hardware accelerators that would enable near real-time classification and extended periods of operations. In this article, we architect, design, implement, and test a handcrafted, hardware Convolutional Neural Network, named BioCNN, optimized for EEG-based emotion detection and other bio-medical applications. The EEG signals are generated using a low-cost, off-the-shelf device, namely, Emotiv Epoc+, and then denoised and pre-processed ahead of their use by BioCNN. For training and testing, BioCNN uses three repositories of emotion classification datasets, including the publicly available DEAP and DREAMER datasets, along with an original dataset collected in-house from 5 healthy subjects using standard visual stimuli. A subject-specific training approach is used under TensorFlow to train BioCNN, which is implemented using the Digilent Atlys Board with a low-cost Spartan-6 FPGA. The experimental results show a competitive energy efficiency of 11 GOps/W, a throughput of 1.65 GOps that is in line with the real-time specification of a wearable device, and a latency of less than 1 ms, which is smaller than the 150 ms required for human interaction times. Its emotion inference accuracy is competitive with the top software-based emotion detectors.},
  keywords={Electroencephalography;Hardware;Feature extraction;Real-time systems;Engines;Training;Machine learning;Emotion recognition;EEG;FPGA;machine learning;hardware accelerator;edge AI;convolutional neural networks;hardware parallelism;pipelining},
  doi={10.1109/ACCESS.2020.3012900},
  ISSN={2169-3536},
  month={},}@ARTICLE{9130719,
  author={Yamanakkanavar, Nagaraj and Lee, Bumshik},
  journal={IEEE Access}, 
  title={Using a Patch-Wise M-Net Convolutional Neural Network for Tissue Segmentation in Brain MRI Images}, 
  year={2020},
  volume={8},
  number={},
  pages={120946-120958},
  abstract={Accurate segmentation of brain tissues, such as gray matter (GM), white matter (WM), and cerebrospinal fluid (CSF), in magnetic resonance imaging (MRI) images, is helpful for the diagnosis of neurological disorders, such as schizophrenia, Alzheimer's disease, and dementia. Studies on MRI-based brain segmentation have received significant attention in recent years based on the non-invasive imaging and good soft-tissue contrast provided by MRI. A number of studies have used conventional machine learning strategies, as well as convolutional neural network approaches. In this paper, we propose a patch-wise M-net architecture for the automatic segmentation of brain MRI images. In the proposed brain segmentation method, slices from a brain MRI scan are divided into non-overlapping patches, which are then fed into an M-net model with corresponding ground-truth patches to train the network, which is composed of two encoder-decoder processes. Dilated convolutional kernels with different sizes are used in the encoder and decoder modules to derive abundant semantic features from brain MRI scans. The proposed patch-wise M-net overcomes the drawbacks of conventional methods and provides greater retention of fine details. The proposed M-net model was trained and tested on the open-access series of imaging studies dataset. The performance was measured quantitatively using the Dice similarity coefficient. Experimental results demonstrate that the proposed method achieves average segmentation accuracies of 94.81% for CSF, 95.44% for GM, and 96.33% for WM, meaning it outperforms state-of-the-art methods.},
  keywords={Image segmentation;Magnetic resonance imaging;Brain modeling;Feature extraction;Three-dimensional displays;Deep learning;Brain MRI;convolutional neural network;M-net;tissue segmentation},
  doi={10.1109/ACCESS.2020.3006317},
  ISSN={2169-3536},
  month={},}@ARTICLE{10829597,
  author={Eltashani, Fatma and Parreno-Centeno, Mario and Cole, James H. and Paulo Papa, João and Costen, Fumie},
  journal={IEEE Access}, 
  title={Brain Age Prediction Using a Lightweight Convolutional Neural Network}, 
  year={2025},
  volume={13},
  number={},
  pages={6750-6763},
  abstract={Much interest has recently been drawn to brain age prediction due to the significant development in machine learning and image processing techniques. Studies based on brain magnetic resonance images showed a strong relationship between the brain ageing process and accelerated brain atrophy, suggesting using brain age prediction models for early diagnosis of neurodegenerative disorders, such as Parkinson’s, Schizophrenia, and Alzheimer’s disease. However, data availability, acquisition protocols diversity and models’ computational complexity remain limiting factors for clinical adoption. This study proposes a low-complexity convolutional neural network (CNN) model that tackles these challenges, focusing on three main aspects: performance accuracy, computational complexity, and adaptability to new, external datasets. We developed a brain-age prediction system using a minimally preprocessed T1-weighted MRI images with a multi-site dataset of healthy individuals covering the whole human lifespan (2251 subjects, age range 6-90 years). We proposed a lighter version of the Simple Fully Convolutional Network (SFCN) that contain only 1.2 million parameters. Computational load was further reduced by cropping the brain images. Finally, we employed transfer learning approach to achieve domain adaptation to external, unseen sites. We demonstrated that leveraging the cropped brain images reduced the computational time for training by 50%, maintaining a comparable accuracy to using the entire brain. The model achieved a Mean Absolute Error (MAE) of 3.557 for the full brain and 4.139 for the cropped images with a Pearson correlation  $ r = 0.988 $  between the full and cropped brain predictions when evaluated on the same test set. Domain adaptation of our model to new external data showed a significant improvement in the prediction performance, reducing MAE from 7.219 to 4.750 for full brain images and from 12.107 to 5.770 for the cropped images. This study is the first to demonstrate comparable prediction accuracy using only a small segment of a 3D full brain MRI scan. Our results show that it is feasible to build lightweight CNN models trained on small-scale, heterogeneous datasets and fine-tuned to new external clinical data, making significant steps toward practical clinical application.},
  keywords={Training;Brain;Brain modeling;Computational modeling;Accuracy;Data models;Adaptation models;Aging;Three-dimensional displays;Magnetic resonance imaging;Biological age estimation;brain imaging;brain ageing;convolutional neural network;deep learning;magnetic resonance imaging},
  doi={10.1109/ACCESS.2025.3526520},
  ISSN={2169-3536},
  month={},}@ARTICLE{9369854,
  author={Ditthapron, Apiwat and Agu, Emmanuel O. and Lammert, Adam C.},
  journal={IEEE Open Journal of Engineering in Medicine and Biology}, 
  title={Privacy-Preserving Deep Speaker Separation for Smartphone-Based Passive Speech Assessment}, 
  year={2021},
  volume={2},
  number={},
  pages={304-313},
  abstract={Goal: Smartphones can be used to passively assess and monitor patients’ speech impairments caused by ailments such as Parkinson’s disease, Traumatic Brain Injury (TBI), Post-Traumatic Stress Disorder (PTSD) and neurodegenerative diseases such as Alzheimer’s disease and dementia. However, passive audio recordings in natural settings often capture the speech of non-target speakers (cross-talk). Consequently, speaker separation, which identifies the target speakers’ speech in audio recordings with two or more speakers’ voices, is a crucial pre-processing step in such scenarios. Prior speech separation methods analyzed raw audio. However, in order to preserve speaker privacy, passively recorded smartphone audio and machine learning-based speech assessment are often performed on derived speech features such as Mel-Frequency Cepstral Coefficients (MFCCs). In this paper, we propose a novel Deep MFCC bAsed SpeaKer Separation (Deep-MASKS). Methods: Deep-MASKS uses an autoencoder to reconstruct MFCC components of an individual’s speech from an i-vector, x-vector or d-vector representation of their speech learned during the enrollment period. Deep-MASKS utilizes a Deep Neural Network (DNN) for MFCC signal reconstructions, which yields a more accurate, higher-order function compared to prior work that utilized a mask. Unlike prior work that operates on utterances, Deep-MASKS operates on continuous audio recordings. Results: Deep-MASKS outperforms baselines, reducing the Mean Squared Error (MSE) of MFCC reconstruction by up to 44% and the number of additional bits required to represent clean speech entropy by 36%.},
  keywords={Mel frequency cepstral coefficient;Smart phones;Feature extraction;Signal reconstruction;Voice activity detection;Medical diagnosis;Speech processing;Mel-Frequency Cepstrum Coefficients (MFCCs);overlapped speech;speaker representation;speech separation},
  doi={10.1109/OJEMB.2021.3063994},
  ISSN={2644-1276},
  month={},}@ARTICLE{10616122,
  author={Kang, Changsu and Wang, Bohyun and Lim, J. S.},
  journal={IEEE Access}, 
  title={Classification Model of Clock Drawing Test Based on Contrastive Learning Using Multi-Channel Features With Channel-Spatial Attention}, 
  year={2024},
  volume={12},
  number={},
  pages={186466-186475},
  abstract={The Clock Drawing Test (CDT) is a professional examination that can detect cognitive impairments, such as Parkinson’s and Alzheimer’s diseases, based on scoring criteria. The pooling layers of a convolutional neural network (CNN) compress features by reducing dimensionality, which tends to focus on a single dominant element. This can be detrimental to compressing information in CDT images, where all elements are significant features. Therefore, in this study, we developed a model that utilizes features obtained from multiple channels to focus on all the elements within an image using channel and spatial attention. We utilized supervised contrastive learning to classify patient and control groups solely from CDT images. The features obtained from the multiple channels of the MCC-net were used to compute contrastive loss and learn representations of the data. Subsequently, a classifier was trained to learn the decision boundaries between the data. When the MCC-net was trained for binary classification, the accuracy, sensitivity, specificity, and area under the curve reached their maximum values of 0.9718, 0.8358, 0.9789, and 0.9700, respectively. As far as our knowledge extends, this study represents the first instance of utilizing supervised contrastive learning, acquiring features from multiple channels, for classifying CDT images, and we achieved superior performance compared to other models. Furthermore, the model visualized the attention clock elements to provide evidence for the inference results and presents the potential of utilizing artificial intelligence to classify CDT images.},
  keywords={Feature extraction;Computational modeling;Clocks;Contrastive learning;Training;Predictive models;Attention mechanisms;Convolutional neural networks;Supervised learning;Attention mechanism;clock drawing test;convolutional neural network;Grad-CAM;supervised contrastive learning},
  doi={10.1109/ACCESS.2024.3436102},
  ISSN={2169-3536},
  month={},}
